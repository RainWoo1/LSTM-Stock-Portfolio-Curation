{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "FEATURE_COLUMNS = [\n",
        "    \"year\", \"month\", \"intrinsic_value\", \"stock_exret\", \"stock_ticker\", \"comp_name\", \"be_me\", \"ni_me\", \"fcf_me\",\n",
        "    \"betadown_252d\", \"ni_ar1\", \"z_score\", \"ebit_sale\", \"at_turnover\",\n",
        "    \"market_equity\", \"roic\", \"bvps\", \"prev_bvps\", \"bvps_change\", \"prev_at_turnover\",\n",
        "    \"at_turnover_change\", \"prev_ni_me\", \"ni_me_change\", \"prev_fcf_me\", \"fcf_me_change\",\n",
        "    \"sin_month\", \"cos_month\"\n",
        "] # 27 - 4 = 23\n",
        "# ignore stock_ticker, comp_name, year, month, ni_me_change, fcf_me_change\n",
        "\n",
        "SEQ_LENGTH = 12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_time_features(df):\n",
        "    \"\"\" Remove `year`, and encode `month` cyclically \"\"\"\n",
        "    if \"year\" not in df.columns or \"month\" not in df.columns:\n",
        "        raise KeyError(\"The dataset does not contain 'year' or 'month' columns.\")\n",
        "\n",
        "    df[\"sin_month\"] = np.sin(2 * np.pi * df[\"month\"] / 12)\n",
        "    df[\"cos_month\"] = np.cos(2 * np.pi * df[\"month\"] / 12)\n",
        "    return df\n",
        "    # return df.drop(columns=[\"year\", \"month\"])  # Remove original time columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize_features(df):\n",
        "    print(df)\n",
        "    feature_columns = [col for col in df.columns if col not in [\"year\", \"month\", \"stock_exret\"]]\n",
        "\n",
        "    # Identify non-numeric columns\n",
        "    # non_numeric_cols = df[feature_columns].select_dtypes(exclude=[np.number]).columns.tolist()\n",
        "    # if non_numeric_cols:\n",
        "    #     print(\"Dropping non-numeric columns:\", non_numeric_cols)\n",
        "    #     df = df.drop(columns=non_numeric_cols)\n",
        "\n",
        "    # Replace NaN and infinite values\n",
        "    # df[feature_columns] = df[feature_columns].replace([np.inf, -np.inf], np.nan)  # Convert inf to NaN\n",
        "    # # Check for infinity values\n",
        "    # print(\"Has Inf values:\\n\", df.isin([np.inf, -np.inf]).sum())\n",
        "\n",
        "    # # Check if any values are too large\n",
        "    # print(\"Max values:\\n\", df.max())\n",
        "\n",
        "    # # Check if any column has extreme variance\n",
        "    # print(\"Std deviation:\\n\", df.std())\n",
        "\n",
        "    df[feature_columns] = df[feature_columns].fillna(0)  # Replace NaN with 0\n",
        "\n",
        "    # Normalize using MinMaxScaler\n",
        "    scaler = MinMaxScaler()\n",
        "    df[feature_columns] = scaler.fit_transform(df[feature_columns])\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_company_data(company):\n",
        "    file_path = f\"data/{company}.csv\"\n",
        "\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"Skipping {company} (No data found)\")\n",
        "        return None, None\n",
        "\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    df = df.fillna(0)  # Replace NaNs with 0\n",
        "    \n",
        "    df = df.drop(columns=[\"stock_ticker\", \"comp_name\", \"ni_me_change\", \"fcf_me_change\",\n",
        "    \"prev_intrinsic_value\",\"next_intrinsic_value\",\"prev_stock_exret\",\"next_stock_exret\",\"prev_be_me\",\n",
        "    \"next_be_me\",\"prev_ni_me\",\"next_ni_me\",\"prev_fcf_me\",\"next_fcf_me\",\"prev_betadown_252d\",\n",
        "    \"next_betadown_252d\",\"prev_ni_ar1\",\"next_ni_ar1\",\"prev_z_score\",\"next_z_score\",\"prev_ebit_sale\",\n",
        "    \"next_ebit_sale\",\"prev_at_turnover\",\"next_at_turnover\",\"prev_market_equity\",\"next_market_equity\"\n",
        "    ,\"prev_bvps\",\"next_bvps\"\n",
        "    ], errors=\"ignore\")\n",
        "    \n",
        "    # Normalize feature columns (excluding year, month, and stock_exret)\n",
        "    df = normalize_features(df)\n",
        "\n",
        "    \n",
        "    if \"year\" not in df.columns or \"month\" not in df.columns:\n",
        "        print(f\"Skipping {company} (Missing 'year' or 'month' column)\")\n",
        "        return None, None\n",
        "\n",
        "    # df = df.sort_values(by=[\"year\", \"month\"]) # already sorted\n",
        "    df_train = df[(df[\"year\"] < 2023) | ((df[\"year\"] == 2023) & (df[\"month\"] < 12))].copy()\n",
        "    df_test = df[(df[\"year\"] == 2023) & (df[\"month\"] == 12)].copy()\n",
        "\n",
        "    df_train = process_time_features(df_train)\n",
        "    df_test = process_time_features(df_test)\n",
        "\n",
        "    return df_train, df_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_lstm_sequences(df, seq_length, company_id):\n",
        "    \"\"\"\n",
        "    Converts dataframe into LSTM sequences for training only.\n",
        "    Uses a rolling window approach: each sequence consists of `seq_length` months,\n",
        "    and the next month's value is the target.\n",
        "    \"\"\"\n",
        "    X_train, Y_train, company_ids_train = [], [], []\n",
        "\n",
        "    # df = df[df['year'] == 2022]\n",
        "    # df = df[df['year'].isin([2022, 2023])]\n",
        "\n",
        "    df = df.drop(columns=['year', 'month'])\n",
        "\n",
        "    print(df)\n",
        "\n",
        "    df_values = df.values  # Convert DataFrame to numpy array\n",
        "    num_samples = len(df_values)\n",
        "\n",
        "    for i in range(num_samples - seq_length):\n",
        "        x_seq = df_values[i:i + seq_length]  # Past `seq_length` months\n",
        "        y_target = df_values[i + seq_length][0]  # Predict next month's `stock_exret`\n",
        "\n",
        "        X_train.append(x_seq)\n",
        "        Y_train.append(y_target)\n",
        "        company_ids_train.append(company_id)\n",
        "\n",
        "    return np.array(X_train), np.array(Y_train), np.array(company_ids_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "class StockLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size, num_companies, embedding_dim):\n",
        "        super(StockLSTM, self).__init__()\n",
        "        \n",
        "        self.input_size = input_size\n",
        "\n",
        "        # Company embedding layer\n",
        "        self.embedding = nn.Embedding(num_companies, embedding_dim)\n",
        "\n",
        "        # LSTM\n",
        "        self.lstm = nn.LSTM(input_size + embedding_dim, hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "        # Fully connected output layer\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, company_ids):\n",
        "        # Convert company IDs to embeddings\n",
        "        company_embedding = self.embedding(company_ids).unsqueeze(1)  # (batch, 1, embedding_dim)\n",
        "        company_embedding = company_embedding.expand(-1, x.shape[1], -1)  # Repeat across time steps\n",
        "\n",
        "        # Concatenate embeddings with financial data\n",
        "        x = torch.cat((x, company_embedding), dim=2)\n",
        "        # print(f\"Model forward input shape: {x.shape}\")  # (batch, seq_length, input_size)\n",
        "\n",
        "        # Pass through LSTM\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "\n",
        "        # Use last time step's output for prediction\n",
        "        output = self.fc(lstm_out[:, -1, :])\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "547\n",
            "{'CSCO': 0, 'PRI': 1, 'UAL': 2, 'TROW': 3, 'ISRG': 4, 'NVR': 5, 'LECO': 6, 'TPR': 7, 'DVN': 8, 'CE': 9, 'NBIX': 10, 'BA': 11, 'VRTX': 12, 'AFG': 13, 'BRK.B': 14, 'GILD': 15, 'MDU': 16, 'MKL': 17, 'HUN': 18, 'TER': 19, 'PODD': 20, 'V': 21, 'QRVO': 22, 'CIEN': 23, 'A': 24, 'MO': 25, 'SWKS': 26, 'MCHP': 27, 'CDNS': 28, 'MSCI': 29, 'CHTR': 30, 'EIX': 31, 'BBY': 32, 'PEGA': 33, 'WBA': 34, 'LVS': 35, 'HCA': 36, 'AJG': 37, 'DTE': 38, 'AN': 39, 'C': 40, 'GWRE': 41, 'BRK.A': 42, 'FAF': 43, 'T': 44, 'CF': 45, 'MGM': 46, 'JLL': 47, 'HUM': 48, 'OSK': 49, 'DAR': 50, 'DLB': 51, 'WU': 52, 'APH': 53, 'ARW': 54, 'SYY': 55, 'MSI': 56, 'BC': 57, 'FCX': 58, 'ADM': 59, 'LH': 60, 'GGG': 61, 'WLK': 62, 'LNT': 63, 'LNC': 64, 'PSX': 65, 'PPC': 66, 'GPN': 67, 'HUBB': 68, 'PPG': 69, 'TECH': 70, 'IQV': 71, 'LNG': 72, 'NOV': 73, 'TNET': 74, 'HAL': 75, 'STZ': 76, 'FLS': 77, 'DXC': 78, 'MSM': 79, 'ADI': 80, 'F': 81, 'HOG': 82, 'ADBE': 83, 'CHH': 84, 'DCI': 85, 'STLD': 86, 'CPRT': 87, 'TDG': 88, 'TFX': 89, 'XPO': 90, 'ULTA': 91, 'SYK': 92, 'TSN': 93, 'GNRC': 94, 'PEP': 95, 'PEG': 96, 'NOW': 97, 'IDA': 98, 'LLY': 99, 'MASI': 100, 'ALNY': 101, 'GPK': 102, 'COST': 103, 'WWD': 104, 'CASY': 105, 'LOW': 106, 'MDLZ': 107, 'BKNG': 108, 'YUMC': 109, 'ZBRA': 110, 'FMC': 111, 'OC': 112, 'XEL': 113, 'AIZ': 114, 'WDAY': 115, 'CW': 116, 'MET': 117, 'APO': 118, 'FTV': 119, 'XRAY': 120, 'INGR': 121, 'FAST': 122, 'TJX': 123, 'SNA': 124, 'HXL': 125, 'MPC': 126, 'BR': 127, 'D': 128, 'MRK': 129, 'NOC': 130, 'KHC': 131, 'IPG': 132, 'BURL': 133, 'UNP': 134, 'ABBV': 135, 'ORCL': 136, 'ECL': 137, 'USFD': 138, 'ETR': 139, 'EBAY': 140, 'SBUX': 141, 'INTU': 142, 'PAYC': 143, 'SAIC': 144, 'IPGP': 145, 'PFGC': 146, 'PG': 147, 'CCK': 148, 'CAT': 149, 'ODFL': 150, 'MCD': 151, 'MNST': 152, 'AMZN': 153, 'MTZ': 154, 'INTC': 155, 'EXP': 156, 'NXST': 157, 'GLW': 158, 'BDX': 159, 'KMI': 160, 'CSGP': 161, 'PWR': 162, 'DXCM': 163, 'HOLX': 164, 'EXPD': 165, 'GM': 166, 'TXN': 167, 'VRSK': 168, 'SJM': 169, 'TMO': 170, 'OXY': 171, 'RL': 172, 'MAN': 173, 'DECK': 174, 'MMM': 175, 'MELI': 176, 'MOS': 177, 'FTNT': 178, 'HSY': 179, 'JNPR': 180, 'DHI': 181, 'MTN': 182, 'ED': 183, 'ES': 184, 'ADSK': 185, 'QLYS': 186, 'IP': 187, 'EXPE': 188, 'KO': 189, 'PCAR': 190, 'WDC': 191, 'PYPL': 192, 'NEE': 193, 'UPS': 194, 'EME': 195, 'EMR': 196, 'MSFT': 197, 'ANSS': 198, 'CTAS': 199, 'BIO': 200, 'UGI': 201, 'CACC': 202, 'CHDN': 203, 'WEC': 204, 'AME': 205, 'ALV': 206, 'IT': 207, 'VRSN': 208, 'EW': 209, 'CMG': 210, 'AWK': 211, 'COO': 212, 'SHW': 213, 'HPQ': 214, 'AMAT': 215, 'MLM': 216, 'BMRN': 217, 'AVY': 218, 'LFUS': 219, 'EVRG': 220, 'EA': 221, 'DE': 222, 'AMD': 223, 'KLAC': 224, 'NDAQ': 225, 'URI': 226, 'WHR': 227, 'KMX': 228, 'SLAB': 229, 'RLI': 230, 'WSO': 231, 'MTCH': 232, 'BIIB': 233, 'NVDA': 234, 'CHRW': 235, 'ROP': 236, 'WSM': 237, 'IDXX': 238, 'POR': 239, 'ENTG': 240, 'EXC': 241, 'HES': 242, 'HD': 243, 'ALB': 244, 'VLO': 245, 'ZTS': 246, 'FDX': 247, 'DG': 248, 'TYL': 249, 'HIG': 250, 'ACM': 251, 'SKX': 252, 'JEF': 253, 'VVV': 254, 'CMS': 255, 'CAG': 256, 'RGLD': 257, 'INCY': 258, 'SCHW': 259, 'HSIC': 260, 'AZO': 261, 'AXP': 262, 'HPE': 263, 'DFS': 264, 'SEE': 265, 'HRL': 266, 'ATR': 267, 'SO': 268, 'ZBH': 269, 'CME': 270, 'XOM': 271, 'AMP': 272, 'BF.B': 273, 'AMG': 274, 'CVX': 275, 'CMCSA': 276, 'PCG': 277, 'PNW': 278, 'ICE': 279, 'SEIC': 280, 'BEN': 281, 'UHS': 282, 'LEA': 283, 'FIVE': 284, 'BKH': 285, 'EMN': 286, 'ROK': 287, 'SIRI': 288, 'PTC': 289, 'ARMK': 290, 'NSC': 291, 'AGCO': 292, 'BWXT': 293, 'NKE': 294, 'FIS': 295, 'FANG': 296, 'VEEV': 297, 'MAS': 298, 'PCTY': 299, 'ETSY': 300, 'TAP': 301, 'MAR': 302, 'XYL': 303, 'CMI': 304, 'TOL': 305, 'MTD': 306, 'HEI': 307, 'GNTX': 308, 'KR': 309, 'GMED': 310, 'IBM': 311, 'BSX': 312, 'COLM': 313, 'LKQ': 314, 'ITW': 315, 'EOG': 316, 'PVH': 317, 'KMB': 318, 'SPGI': 319, 'NEM': 320, 'LULU': 321, 'BRKR': 322, 'WEX': 323, 'TTD': 324, 'EL': 325, 'GS': 326, 'OMF': 327, 'GD': 328, 'CNP': 329, 'PM': 330, 'MCO': 331, 'TXRH': 332, 'CLX': 333, 'CAH': 334, 'HQY': 335, 'PANW': 336, 'EXEL': 337, 'MMS': 338, 'MPWR': 339, 'DGX': 340, 'DIS': 341, 'RS': 342, 'CBRE': 343, 'GE': 344, 'HII': 345, 'CGNX': 346, 'LDOS': 347, 'ALL': 348, 'ERIE': 349, 'SRPT': 350, 'LPX': 351, 'WEN': 352, 'ITT': 353, 'ALGN': 354, 'NFLX': 355, 'KNX': 356, 'LEN': 357, 'WST': 358, 'GWW': 359, 'TREX': 360, 'LII': 361, 'EEFT': 362, 'MANH': 363, 'FICO': 364, 'CVS': 365, 'AOS': 366, 'FE': 367, 'ABT': 368, 'OMC': 369, 'COF': 370, 'TSCO': 371, 'ASGN': 372, 'PH': 373, 'AYI': 374, 'JBHT': 375, 'TSLA': 376, 'MOH': 377, 'ATO': 378, 'COP': 379, 'DHR': 380, 'MAT': 381, 'BFAM': 382, 'CNC': 383, 'MCK': 384, 'TXT': 385, 'FDS': 386, 'BERY': 387, 'AKAM': 388, 'ROL': 389, 'RMD': 390, 'WRB': 391, 'OLED': 392, 'GOOGL': 393, 'NFG': 394, 'CACI': 395, 'BRO': 396, 'PAG': 397, 'ANET': 398, 'PAYX': 399, 'ALK': 400, 'DRI': 401, 'ILMN': 402, 'AAL': 403, 'CNA': 404, 'LSTR': 405, 'MMC': 406, 'SSNC': 407, 'TTEK': 408, 'CLH': 409, 'SF': 410, 'POOL': 411, 'FFIV': 412, 'VMC': 413, 'TTC': 414, 'MKTX': 415, 'SRE': 416, 'ORLY': 417, 'SNPS': 418, 'GOOG': 419, 'EPAM': 420, 'NYT': 421, 'BAH': 422, 'NDSN': 423, 'YUM': 424, 'LYV': 425, 'PFE': 426, 'OGE': 427, 'DUK': 428, 'REGN': 429, 'CL': 430, 'VFC': 431, 'VZ': 432, 'PSTG': 433, 'FSLR': 434, 'AMGN': 435, 'JBL': 436, 'VST': 437, 'JKHY': 438, 'ADP': 439, 'SWX': 440, 'CSL': 441, 'ON': 442, 'RSG': 443, 'IFF': 444, 'SLGN': 445, 'RDN': 446, 'TRMB': 447, 'QCOM': 448, 'GIS': 449, 'PII': 450, 'PHM': 451, 'ROST': 452, 'LUV': 453, 'LW': 454, 'MS': 455, 'CPB': 456, 'OKE': 457, 'MSA': 458, 'MIDD': 459, 'SYF': 460, 'TKR': 461, 'SNX': 462, 'CHD': 463, 'MHK': 464, 'SCI': 465, 'DAL': 466, 'CHE': 467, 'K': 468, 'AFL': 469, 'CSX': 470, 'NI': 471, 'RGA': 472, 'EHC': 473, 'RJF': 474, 'IONS': 475, 'UNH': 476, 'PRU': 477, 'GPC': 478, 'ALLY': 479, 'VOYA': 480, 'WMB': 481, 'UTHR': 482, 'DVA': 483, 'POST': 484, 'AIG': 485, 'MA': 486, 'TRU': 487, 'HON': 488, 'NWSA': 489, 'TTWO': 490, 'AES': 491, 'SON': 492, 'LANC': 493, 'TGT': 494, 'AAPL': 495, 'MKC': 496, 'TDY': 497, 'ASH': 498, 'APD': 499, 'SCCO': 500, 'AEE': 501, 'HLT': 502, 'DLTR': 503, 'HAS': 504, 'FND': 505, 'TMUS': 506, 'PEN': 507, 'WMT': 508, 'RPM': 509, 'NTAP': 510, 'BAX': 511, 'LMT': 512, 'LPLA': 513, 'UNM': 514, 'KEYS': 515, 'BMY': 516, 'WYNN': 517, 'RHI': 518, 'EFX': 519, 'NUE': 520, 'PKG': 521, 'GDDY': 522, 'WAB': 523, 'CTSH': 524, 'FNF': 525, 'SWK': 526, 'CRL': 527, 'MU': 528, 'TRV': 529, 'MKSI': 530, 'AEP': 531, 'CI': 532, 'CDW': 533, 'JNJ': 534, 'WM': 535, 'DOV': 536, 'Z': 537, 'FLO': 538, 'CRM': 539, 'PGR': 540, 'WAT': 541, 'IEX': 542, 'BWA': 543, 'LRCX': 544, 'BLK': 545, 'PPL': 546}\n"
          ]
        }
      ],
      "source": [
        "company_files = [f.split(\".csv\")[0] for f in os.listdir(\"data\") if f.endswith(\".csv\")]  # 2501 companies\n",
        "print(len(company_files))\n",
        "company_to_id = {comp: idx for idx, comp in enumerate(company_files)}\n",
        "print(company_to_id)\n",
        "\n",
        "# Hyperparameters\n",
        "num_epochs = 200\n",
        "batch_size = 64\n",
        "\n",
        "# Model parameters\n",
        "feature_input_size = len(FEATURE_COLUMNS)  # This should match actual features before embedding\n",
        "embedding_dim = 24  # sqrt(num_companies) 547 companies\n",
        "input_size = 17  # feature_input_size(25) + embedding_dim(50) = 75 17\n",
        "\n",
        "hidden_size = 64\n",
        "num_layers = 2\n",
        "output_size = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combine data from all companies\n",
        "all_X_train = []\n",
        "all_Y_train = []\n",
        "all_company_ids = []\n",
        "\n",
        "for company in company_files:\n",
        "    company_id = company_to_id[company]\n",
        "    print(company_id, \": \", company)\n",
        "\n",
        "    df_train, df_test = load_company_data(company)\n",
        "\n",
        "    if df_train is None:\n",
        "        continue\n",
        "\n",
        "    X_train_tensor, Y_train_tensor, company_train_tensor = create_lstm_sequences(df_train, 5, company_id)\n",
        "\n",
        "    all_X_train.append(torch.tensor(X_train_tensor, dtype=torch.float32))\n",
        "    all_Y_train.append(torch.tensor(Y_train_tensor, dtype=torch.float32).unsqueeze(1))\n",
        "    all_company_ids.append(torch.tensor(company_train_tensor, dtype=torch.long))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = StockLSTM(input_size, hidden_size, num_layers, output_size, len(company_files), embedding_dim).to(device)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Concatenate all data\n",
        "X_train_tensor = torch.cat(all_X_train, dim=0)\n",
        "Y_train_tensor = torch.cat(all_Y_train, dim=0)\n",
        "company_train_tensor = torch.cat(all_company_ids, dim=0)\n",
        "\n",
        "# Create DataLoader\n",
        "train_dataset = TensorDataset(X_train_tensor, company_train_tensor, Y_train_tensor)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Training\n",
        "train_losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch_X, batch_company, batch_Y in train_dataloader:\n",
        "        batch_X, batch_company, batch_Y = batch_X.to(device), batch_company.to(device), batch_Y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(batch_X, batch_company).squeeze()\n",
        "        loss = criterion(predictions, batch_Y)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    train_losses.append(avg_loss)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_loss:.6f}\")\n",
        "\n",
        "# Plot Loss Curve\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(1, num_epochs + 1), train_losses, marker='o', linestyle='-')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Training Loss\")\n",
        "plt.title(\"Training Loss Over Epochs\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CSCO\n",
        "torch.save(model.state_dict(), \"trained_stock_lstm.pth\")\n",
        "print(\"\\nModel saved!\")\n",
        "\n",
        "# Testing (Predict Dec 2023 & Compare to Actual)\n",
        "\n",
        "test_company = input(\"\\nEnter company name for testing: \")\n",
        "df_train, df_test = load_company_data(test_company)\n",
        "\n",
        "print(df_test)\n",
        "\n",
        "# df_test is already processed because of load_company_data\n",
        "\n",
        "if df_test is None or df_test.empty:\n",
        "    print(f\" No test data available for {test_company}.\")\n",
        "else:\n",
        "    latest_financials = df_test.iloc[-1].to_dict()\n",
        "\n",
        "    input_df = pd.DataFrame([latest_financials], columns=FEATURE_COLUMNS)\n",
        "    print(input_df)\n",
        "\n",
        "    input_df = input_df.drop(columns=[\"stock_ticker\", \"comp_name\", \"ni_me_change\", \"fcf_me_change\",\n",
        "    \"prev_intrinsic_value\",\"next_intrinsic_value\",\"prev_stock_exret\",\"next_stock_exret\",\"prev_be_me\",\n",
        "    \"next_be_me\",\"prev_ni_me\",\"next_ni_me\",\"prev_fcf_me\",\"next_fcf_me\",\"prev_betadown_252d\",\n",
        "    \"next_betadown_252d\",\"prev_ni_ar1\",\"next_ni_ar1\",\"prev_z_score\",\"next_z_score\",\"prev_ebit_sale\",\n",
        "    \"next_ebit_sale\",\"prev_at_turnover\",\"next_at_turnover\",\"prev_market_equity\",\"next_market_equity\"\n",
        "    ,\"prev_bvps\",\"next_bvps\"\n",
        "    ], errors=\"ignore\")\n",
        "    \n",
        "    input_df = input_df.fillna(0)  # in case\n",
        "    # input_df = normalize_features(input_df)\n",
        "    print(input_df)\n",
        "    \n",
        "    input_tensor = torch.tensor(input_df.values, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "\n",
        "    print(input_tensor.shape)\n",
        "    \n",
        "    test_company_id = torch.tensor([company_to_id[test_company]], dtype=torch.long).to(device)\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        predicted_return = model(input_tensor, test_company_id).item()\n",
        "\n",
        "    actual_return = df_test[\"stock_exret\"].values[0]\n",
        "\n",
        "    print(f\"\\nPredicted: {predicted_return:.4f} | Actual: {actual_return:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
